import pandas as pdimport matplotlib.pyplot as pltfrom sklearn.cluster import DBSCANfrom sklearn.ensemble import IsolationForestimport numpy as np# ========== 数据加载与预处理 ==========df = pd.read_csv('processed_transactions.csv')df['timeStamp'] = pd.to_datetime(df['timeStamp'])df['value'] = df['value'].astype(float)# 提取高峰期交易数据（2022.5.1 - 2022.5.15）start_date = pd.to_datetime('2022-05-01')end_date = pd.to_datetime('2022-05-15')peak_data = df[(df['timeStamp'] >= start_date) & (df['timeStamp'] <= end_date)]print(f"高峰期交易记录总数: {len(peak_data)}")# ========== 方法 1：基于统计规则的大额交易检测 ==========# 检测交易金额异常大的记录（Top 1%）threshold = peak_data['value'].quantile(0.99)  # 99%分位数large_transactions = peak_data[peak_data['value'] > threshold]print(f"大额交易数量: {len(large_transactions)}")# 可视化交易金额分布（标记大额交易）plt.figure(figsize=(10, 6))plt.hist(peak_data['value'], bins=50, log=True, color='blue', alpha=0.7)plt.axvline(threshold, color='red', linestyle='dashed', linewidth=1, label='99% Threshold')plt.title('Transaction Value Distribution (Log Scale)', fontsize=16)plt.xlabel('Transaction Value', fontsize=12)plt.ylabel('Frequency (Log Scale)', fontsize=12)plt.legend()plt.grid(alpha=0.3)plt.tight_layout()plt.show()# 输出大额交易的详情print("Top 1% Large Transactions:")print(large_transactions[['from_address', 'to_address', 'value', 'timeStamp']])# ========== 方法 2：基于频率的高频交易检测 ==========# 统计每个地址的交易频率frequency_threshold = 100  # 设定频率阈值address_frequencies = peak_data['from_address'].value_counts()high_frequency_addresses = address_frequencies[address_frequencies > frequency_threshold]print(f"高频交易地址数量: {len(high_frequency_addresses)}")# 可视化高频交易地址plt.figure(figsize=(10, 6))high_frequency_addresses.head(10).plot(kind='bar', color='orange', alpha=0.7)plt.title('Top High-Frequency Addresses', fontsize=16)plt.xlabel('Address', fontsize=12)plt.ylabel('Transaction Count', fontsize=12)plt.xticks(rotation=45)plt.tight_layout()plt.show()# 输出高频地址及其交易频率print("High-Frequency Addresses:")print(high_frequency_addresses)# ========== 方法 3：基于聚类的异常检测 (DBSCAN) ==========# 仅选择需要的特征（交易金额）X = peak_data[['value']].values# 使用 DBSCAN 进行聚类dbscan = DBSCAN(eps=0.5, min_samples=10)  # 根据数据调整参数labels = dbscan.fit_predict(np.log1p(X))  # 使用对数变换以减少金额偏差# 标记异常peak_data['dbscan_label'] = labelsanomalies_dbscan = peak_data[peak_data['dbscan_label'] == -1]print(f"DBSCAN 检测到的异常交易数量: {len(anomalies_dbscan)}")# 可视化 DBSCAN 聚类结果plt.figure(figsize=(10, 6))plt.scatter(peak_data.index, peak_data['value'], c=peak_data['dbscan_label'], cmap='viridis', alpha=0.7)plt.colorbar(label='Cluster Label')plt.title('DBSCAN Anomaly Detection', fontsize=16)plt.xlabel('Transaction Index', fontsize=12)plt.ylabel('Transaction Value', fontsize=12)plt.grid(alpha=0.3)plt.tight_layout()plt.show()# 输出 DBSCAN 检测到的异常交易print("DBSCAN Detected Anomalies:")print(anomalies_dbscan[['from_address', 'to_address', 'value', 'timeStamp']])# ========== 方法 4：基于孤立森林 (Isolation Forest) ==========# 使用 Isolation Forest 进行异常检测clf = IsolationForest(contamination=0.01, random_state=42)  # 1% 的异常比例X_scaled = np.log1p(X)  # 对数变换clf.fit(X_scaled)# 预测异常 (-1 为异常)peak_data['isolation_label'] = clf.predict(X_scaled)anomalies_isolation = peak_data[peak_data['isolation_label'] == -1]print(f"Isolation Forest 检测到的异常交易数量: {len(anomalies_isolation)}")# 可视化 Isolation Forest 检测结果plt.figure(figsize=(10, 6))plt.scatter(peak_data.index, peak_data['value'], c=peak_data['isolation_label'], cmap='coolwarm', alpha=0.7)plt.colorbar(label='Anomaly Label')plt.title('Isolation Forest Anomaly Detection', fontsize=16)plt.xlabel('Transaction Index', fontsize=12)plt.ylabel('Transaction Value', fontsize=12)plt.grid(alpha=0.3)plt.tight_layout()plt.show()# 输出 Isolation Forest 检测到的异常交易print("Isolation Forest Detected Anomalies:")print(anomalies_isolation[['from_address', 'to_address', 'value', 'timeStamp']])